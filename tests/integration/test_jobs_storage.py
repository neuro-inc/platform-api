from datetime import datetime, timedelta
from itertools import islice
from pathlib import PurePath
from typing import Any, Dict, List, Optional, Tuple

import aioredis
import pytest
from asyncpg.pool import Pool
from yarl import URL

from platform_api.config import PostgresConfig
from platform_api.config_factory import EnvironConfigFactory
from platform_api.orchestrator.job import (
    JobRecord,
    JobStatusHistory,
    JobStatusItem,
    current_datetime_factory,
)
from platform_api.orchestrator.job_request import (
    Container,
    ContainerResources,
    ContainerVolume,
    JobError,
    JobRequest,
    JobStatus,
)
from platform_api.orchestrator.jobs_storage import (
    JobFilter,
    JobsStorage,
    JobStorageJobFoundError,
    JobStorageTransactionError,
    RedisJobsStorage,
)
from platform_api.orchestrator.jobs_storage.postgres import PostgresJobsStorage
from platform_api.postgres import MigrationRunner, create_postgres_pool
from platform_api.redis import RedisConfig
from tests.conftest import not_raises, random_str


class TestJobsStorage:
    @pytest.fixture(params=["redis", "postgres"])
    def storage(
        self, request: Any, redis_client: aioredis.Redis, postgres_pool: Pool
    ) -> JobsStorage:
        if request.param == "redis":
            return RedisJobsStorage(redis_client)
        if request.param == "postgres":
            return PostgresJobsStorage(postgres_pool)
        raise Exception(f"Unknown job storage engine {request.param}.")

    def _create_job_request(self, with_gpu: bool = False) -> JobRequest:
        if with_gpu:
            resources = ContainerResources(
                cpu=0.1, memory_mb=256, gpu=1, gpu_model_id="nvidia-tesla-k80"
            )
        else:
            resources = ContainerResources(cpu=0.1, memory_mb=256)
        container = Container(image="ubuntu", command="sleep 5", resources=resources)
        return JobRequest.create(container)

    def _create_job(
        self, cluster_name: str = "test-cluster", **kwargs: Any
    ) -> JobRecord:
        return JobRecord.create(
            request=self._create_job_request(), cluster_name=cluster_name, **kwargs
        )

    def _create_pending_job(
        self, owner: str = "compute", job_name: Optional[str] = None, **kwargs: Any
    ) -> JobRecord:
        return self._create_job(owner=owner, name=job_name, **kwargs)

    def _create_running_job(
        self, owner: str = "compute", job_name: Optional[str] = None, **kwargs: Any
    ) -> JobRecord:
        kwargs.setdefault("materialized", True)
        return self._create_job(
            name=job_name, owner=owner, status=JobStatus.RUNNING, **kwargs
        )

    def _create_succeeded_job(
        self, owner: str = "compute", job_name: Optional[str] = None, **kwargs: Any
    ) -> JobRecord:
        kwargs.setdefault("materialized", True)
        return self._create_job(
            name=job_name, status=JobStatus.SUCCEEDED, owner=owner, **kwargs
        )

    def _create_failed_job(
        self, owner: str = "compute", job_name: Optional[str] = None, **kwargs: Any
    ) -> JobRecord:
        kwargs.setdefault("materialized", True)
        return self._create_job(
            name=job_name, status=JobStatus.FAILED, owner=owner, **kwargs
        )

    def _create_cancelled_job(
        self, owner: str = "compute", job_name: Optional[str] = None, **kwargs: Any
    ) -> JobRecord:
        return self._create_job(
            name=job_name, status=JobStatus.CANCELLED, owner=owner, **kwargs
        )

    @pytest.mark.asyncio
    async def test_set_get(self, storage: JobsStorage) -> None:
        original_job = self._create_pending_job()
        await storage.set_job(original_job)

        job = await storage.get_job(original_job.id)
        assert job.id == original_job.id
        assert job.status == original_job.status

    @pytest.mark.asyncio
    async def test_get_last_created_job_id__no_job_updated(
        self, redis_client: aioredis.Redis
    ) -> None:
        job = self._create_pending_job(owner="me", job_name="jobname1")
        storage = RedisJobsStorage(redis_client)
        job.status = JobStatus.RUNNING

        # do not put this job into redis

        # check that job exists in database:
        with pytest.raises(JobError, match=f"no such job {job.id}"):
            await storage.get_job(job.id)

        assert job.name
        job_id_last_created = await storage.get_last_created_job_id(job.owner, job.name)
        assert job_id_last_created is None

    @pytest.mark.asyncio
    async def test_get_last_created_job_id__is_job_creation_false(
        self, redis_client: aioredis.Redis
    ) -> None:

        job = self._create_pending_job(owner="me", job_name="jobname1")
        storage = RedisJobsStorage(redis_client)
        job.status = JobStatus.RUNNING
        await storage.update_job_atomic(job, is_job_creation=False)

        # check that job exists in database:
        job_read = await storage.get_job(job.id)
        assert job_read.to_primitive() == job.to_primitive()

        assert job.name
        job_id_last_created = await storage.get_last_created_job_id(job.owner, job.name)
        assert job_id_last_created is None

    @pytest.mark.asyncio
    async def test_get_last_created_job_id__is_job_creation_true(
        self, redis_client: aioredis.Redis
    ) -> None:
        job = self._create_pending_job(owner="me", job_name="jobname1")
        storage = RedisJobsStorage(redis_client)
        job.status = JobStatus.RUNNING
        await storage.update_job_atomic(job, is_job_creation=True)

        # check that job exists in database:
        job_read = await storage.get_job(job.id)
        assert job_read.to_primitive() == job.to_primitive()

        assert job.name
        job_id_last_created = await storage.get_last_created_job_id(job.owner, job.name)
        assert job_id_last_created == job.id

    @pytest.mark.asyncio
    async def test_get_last_created_job_id__multiple_jobs_order_preserves(
        self, redis_client: aioredis.Redis
    ) -> None:
        storage = RedisJobsStorage(redis_client)
        owner = "me"
        name = "jobnamename"

        job1 = self._create_pending_job(owner=owner, job_name=name)
        await storage.update_job_atomic(job1, is_job_creation=True)

        job2 = self._create_failed_job(owner=owner, job_name=name)
        await storage.update_job_atomic(job2, is_job_creation=True)

        job3 = self._create_succeeded_job(owner=owner, job_name=name)
        await storage.update_job_atomic(job3, is_job_creation=True)

        job_id_last_created = await storage.get_last_created_job_id(owner, name)
        assert job_id_last_created == job3.id

    @pytest.mark.asyncio
    async def test_try_create_job__no_name__ok(self, storage: JobsStorage) -> None:

        pending_job = self._create_pending_job()
        async with storage.try_create_job(pending_job) as job:
            assert pending_job.status == JobStatus.PENDING
            assert job.id == pending_job.id
            job.status = JobStatus.RUNNING
        result_job = await storage.get_job(pending_job.id)
        assert result_job.status == JobStatus.RUNNING

        running_job = self._create_running_job()
        async with storage.try_create_job(running_job) as job:
            assert running_job.status == JobStatus.RUNNING
            assert job.id == running_job.id
            job.status = JobStatus.SUCCEEDED
        result_job = await storage.get_job(running_job.id)
        assert result_job.status == JobStatus.SUCCEEDED

    @pytest.mark.asyncio
    async def test_try_create_job__no_name__job_changed_while_creation(
        self, storage: JobsStorage
    ) -> None:
        job = self._create_pending_job()

        # process-1
        with pytest.raises(
            JobStorageTransactionError, match=f"Job {{id={job.id}}} has changed"
        ):
            async with storage.try_create_job(job) as first_job:
                # value in orchestrator: PENDING, value in redis: None
                assert first_job.status == JobStatus.PENDING
                first_job.status = JobStatus.SUCCEEDED
                # value in orchestrator: SUCCEEDED, value in redis: None

                # process-2
                with not_raises(JobStorageTransactionError):
                    async with storage.try_create_job(job) as second_job:
                        # value in orchestrator: succeeded, value in redis: None
                        assert second_job.status == JobStatus.SUCCEEDED
                        second_job.status = JobStatus.RUNNING
                        # value in orchestrator: FAILED, value in redis: None
                        # now status FAILED is written into the redis by process-2

                # now status SUCCEEDED fails to be written into the redis by process-1

                # now jobs-service catches transaction exception thrown
                # by process-1 and deletes the newly created by process-1 job.

        result_job = await storage.get_job(job.id)
        assert result_job.status == JobStatus.RUNNING

    @pytest.mark.asyncio
    async def test_try_create_job__different_name_same_owner__ok(
        self, storage: JobsStorage
    ) -> None:

        owner = "test-user-1"
        job_name_1 = "some-test-job-name-1"
        job_name_2 = "some-test-job-name-2"

        job_1 = self._create_pending_job(job_name=job_name_1, owner=owner)
        job_2 = self._create_running_job(job_name=job_name_2, owner=owner)

        async with storage.try_create_job(job_1):
            pass
        async with storage.try_create_job(job_2):
            pass

        job = await storage.get_job(job_1.id)
        assert job.id == job_1.id
        assert job.status == JobStatus.PENDING

        job = await storage.get_job(job_2.id)
        assert job.id == job_2.id
        assert job.status == JobStatus.RUNNING

    @pytest.mark.asyncio
    async def test_try_create_job__same_name_different_owner__ok(
        self, storage: JobsStorage
    ) -> None:

        owner_1 = "test-user-1"
        job_name_1 = "some-test-job-name-1"
        owner_2 = "test-user-2"
        job_name_2 = "some-test-job-name-2"

        job_1 = self._create_pending_job(job_name=job_name_1, owner=owner_1)
        job_2 = self._create_running_job(job_name=job_name_2, owner=owner_2)

        async with storage.try_create_job(job_1):
            pass
        async with storage.try_create_job(job_2):
            pass

        job = await storage.get_job(job_1.id)
        assert job.id == job_1.id
        assert job.status == JobStatus.PENDING

        job = await storage.get_job(job_2.id)
        assert job.id == job_2.id
        assert job.status == JobStatus.RUNNING

    @pytest.mark.asyncio
    @pytest.mark.parametrize("first_job_status", [JobStatus.PENDING, JobStatus.RUNNING])
    async def test_try_create_job__same_name_with_an_active_job__conflict(
        self, storage: JobsStorage, first_job_status: JobStatus
    ) -> None:

        owner = "test-user"
        job_name = "some-test-job-name"

        first_job = self._create_job(
            name=job_name, status=first_job_status, owner=owner
        )
        async with storage.try_create_job(first_job):
            pass
        job = await storage.get_job(first_job.id)
        assert job.id == first_job.id
        assert job.status == first_job_status

        second_job = self._create_pending_job(job_name=job_name, owner=owner)
        with pytest.raises(
            JobStorageJobFoundError,
            match=f"job with name '{job_name}' and owner '{first_job.owner}'"
            f" already exists: '{first_job.id}'",
        ):
            async with storage.try_create_job(second_job):
                pass

    @pytest.mark.asyncio
    @pytest.mark.parametrize(
        "first_job_status", [JobStatus.SUCCEEDED, JobStatus.FAILED]
    )
    async def test_try_create_job__same_name_with_a_terminated_job__ok(
        self, storage: JobsStorage, first_job_status: JobStatus
    ) -> None:
        owner = "test-user"
        job_name = "some-test-job-name"

        first_job = self._create_job(
            name=job_name, status=first_job_status, owner=owner
        )
        second_job = self._create_pending_job(owner=owner, job_name=job_name)

        # create first job:
        async with storage.try_create_job(first_job):
            pass
        job = await storage.get_job(first_job.id)
        assert job.id == first_job.id
        assert job.status == first_job_status

        # create second job:
        async with storage.try_create_job(second_job):
            pass
        job = await storage.get_job(second_job.id)
        assert job.id == second_job.id
        assert job.status == JobStatus.PENDING

    @pytest.mark.asyncio
    async def test_try_create_job_with_tags(self, storage: JobsStorage) -> None:

        tags = ["tag1", "tag2"]
        job = self._create_job(tags=tags)
        async with storage.try_create_job(job) as job:
            assert job.id == job.id
            assert job.tags == tags

        result_job = await storage.get_job(job.id)
        assert result_job.tags == tags

    @pytest.mark.asyncio
    async def test_get_non_existent(self, storage: JobsStorage) -> None:
        with pytest.raises(JobError, match="no such job unknown"):
            await storage.get_job("unknown")

    @pytest.mark.asyncio
    async def test_get_all_no_filter_empty_result(self, storage: JobsStorage) -> None:

        jobs = await storage.get_all_jobs()
        assert not jobs

    @pytest.mark.asyncio
    async def test_get_all_no_filter_single_job(self, storage: JobsStorage) -> None:
        original_job = self._create_pending_job()
        await storage.set_job(original_job)

        jobs = await storage.get_all_jobs()
        assert len(jobs) == 1
        job = jobs[0]
        assert job.id == original_job.id
        assert job.status == original_job.status

    @pytest.mark.asyncio
    async def test_get_all_no_filter_multiple_jobs(self, storage: JobsStorage) -> None:
        original_jobs = [
            self._create_pending_job(job_name="jobname1"),
            self._create_running_job(job_name="jobname2"),
            self._create_succeeded_job(job_name="jobname3"),
            self._create_failed_job(job_name="jobname3"),
        ]
        for job in original_jobs:
            async with storage.try_create_job(job):
                pass

        jobs = await storage.get_all_jobs()

        job_reprs = sorted(
            [job.to_primitive() for job in jobs], key=lambda job: job["id"]
        )
        original_job_reprs = sorted(
            [job.to_primitive() for job in original_jobs], key=lambda job: job["id"]
        )
        assert job_reprs == original_job_reprs

    @pytest.mark.asyncio
    async def test_get_all_filter_by_status(self, storage: JobsStorage) -> None:
        pending_job = self._create_pending_job()
        running_job = self._create_running_job()
        succeeded_job = self._create_succeeded_job()
        await storage.set_job(pending_job)
        await storage.set_job(running_job)
        await storage.set_job(succeeded_job)

        jobs = await storage.get_all_jobs()
        job_ids = [job.id for job in jobs]
        assert job_ids == [pending_job.id, running_job.id, succeeded_job.id]

        jobs = await storage.get_all_jobs(reverse=True)
        job_ids = [job.id for job in jobs]
        assert job_ids == [succeeded_job.id, running_job.id, pending_job.id]

        jobs = await storage.get_all_jobs(limit=2)
        job_ids = [job.id for job in jobs]
        assert job_ids == [pending_job.id, running_job.id]

        jobs = await storage.get_all_jobs(reverse=True, limit=2)
        job_ids = [job.id for job in jobs]
        assert job_ids == [succeeded_job.id, running_job.id]

        filters = JobFilter(statuses={JobStatus.FAILED})
        jobs = await storage.get_all_jobs(filters)
        job_ids = [job.id for job in jobs]
        assert job_ids == []

        filters = JobFilter(statuses={JobStatus.SUCCEEDED, JobStatus.RUNNING})
        jobs = await storage.get_all_jobs(filters)
        job_ids = [job.id for job in jobs]
        assert job_ids == [running_job.id, succeeded_job.id]

        jobs = await storage.get_all_jobs(filters, reverse=True)
        job_ids = [job.id for job in jobs]
        assert job_ids == [succeeded_job.id, running_job.id]

        jobs = await storage.get_all_jobs(filters, limit=1)
        job_ids = [job.id for job in jobs]
        assert job_ids == [running_job.id]

        jobs = await storage.get_all_jobs(filters, reverse=True, limit=1)
        job_ids = [job.id for job in jobs]
        assert job_ids == [succeeded_job.id]

    @pytest.mark.asyncio
    async def test_get_all_filter_by_tags(self, storage: JobsStorage) -> None:
        job1 = self._create_job(tags=["t1"])
        job2 = self._create_job(tags=["t1", "t2"])
        job3 = self._create_job(tags=["t2"])
        job4 = self._create_job(tags=["t3"])

        await storage.set_job(job1)
        await storage.set_job(job2)
        await storage.set_job(job3)
        await storage.set_job(job4)

        jobs = await storage.get_all_jobs()
        job_ids = [job.id for job in jobs]
        assert job_ids == [job1.id, job2.id, job3.id, job4.id]

        filters = JobFilter(tags={"t1"})
        jobs = await storage.get_all_jobs(filters)
        job_ids = [job.id for job in jobs]
        assert job_ids == [job1.id, job2.id]

        filters = JobFilter(tags={"t1", "t2"})
        jobs = await storage.get_all_jobs(filters)
        job_ids = [job.id for job in jobs]
        assert job_ids == [job2.id]

        jobs = await storage.get_all_jobs(filters, limit=1)
        job_ids = [job.id for job in jobs]
        assert job_ids == [job2.id]

        jobs = await storage.get_all_jobs(filters, limit=1, reverse=True)
        job_ids = [job.id for job in jobs]
        assert job_ids == [job2.id]

        filters = JobFilter(tags={"t3"})
        jobs = await storage.get_all_jobs(filters)
        job_ids = [job.id for job in jobs]
        assert job_ids == [job4.id]

        filters = JobFilter(tags={"t1", "t2", "t3"})
        jobs = await storage.get_all_jobs(filters)
        job_ids = [job.id for job in jobs]
        assert not job_ids

    @pytest.mark.asyncio
    @pytest.mark.parametrize("statuses", [(), (JobStatus.PENDING, JobStatus.RUNNING)])
    async def test_get_all_filter_by_date_range(
        self, statuses: Tuple[JobStatus, ...], storage: JobsStorage
    ) -> None:
        t1 = current_datetime_factory()
        job1 = self._create_job()
        t2 = current_datetime_factory()
        job2 = self._create_job()
        t3 = current_datetime_factory()
        job3 = self._create_job()
        t4 = current_datetime_factory()

        await storage.set_job(job1)
        await storage.set_job(job2)
        await storage.set_job(job3)

        job_filter = JobFilter(since=t1, until=t4)
        job_ids = [job.id for job in await storage.get_all_jobs(job_filter)]
        assert job_ids == [job1.id, job2.id, job3.id]

        job_ids = [
            job.id for job in await storage.get_all_jobs(job_filter, reverse=True)
        ]
        assert job_ids == [job3.id, job2.id, job1.id]

        job_ids = [job.id for job in await storage.get_all_jobs(job_filter, limit=2)]
        assert job_ids == [job1.id, job2.id]

        job_ids = [
            job.id
            for job in await storage.get_all_jobs(job_filter, reverse=True, limit=2)
        ]
        assert job_ids == [job3.id, job2.id]

        job_filter = JobFilter(since=t2)
        job_ids = [job.id for job in await storage.get_all_jobs(job_filter)]
        assert job_ids == [job2.id, job3.id]

        job_ids = [
            job.id for job in await storage.get_all_jobs(job_filter, reverse=True)
        ]
        assert job_ids == [job3.id, job2.id]

        job_ids = [job.id for job in await storage.get_all_jobs(job_filter, limit=1)]
        assert job_ids == [job2.id]

        job_ids = [
            job.id
            for job in await storage.get_all_jobs(job_filter, reverse=True, limit=1)
        ]
        assert job_ids == [job3.id]

        job_filter = JobFilter(until=t2)
        job_ids = [job.id for job in await storage.get_all_jobs(job_filter)]
        assert job_ids == [job1.id]

        job_filter = JobFilter(since=t3)
        job_ids = [job.id for job in await storage.get_all_jobs(job_filter)]
        assert job_ids == [job3.id]

        job_filter = JobFilter(until=t3)
        job_ids = [job.id for job in await storage.get_all_jobs(job_filter)]
        assert job_ids == [job1.id, job2.id]

        job_ids = [
            job.id for job in await storage.get_all_jobs(job_filter, reverse=True)
        ]
        assert job_ids == [job2.id, job1.id]

        job_ids = [job.id for job in await storage.get_all_jobs(job_filter, limit=1)]
        assert job_ids == [job1.id]

        job_ids = [
            job.id
            for job in await storage.get_all_jobs(job_filter, reverse=True, limit=1)
        ]
        assert job_ids == [job2.id]

        job_filter = JobFilter(since=t2, until=t3)
        job_ids = [job.id for job in await storage.get_all_jobs(job_filter)]
        assert job_ids == [job2.id]

        job_filter = JobFilter(since=t3, until=t2)
        job_ids = [job.id for job in await storage.get_all_jobs(job_filter)]
        assert job_ids == []

    async def prepare_filtering_test(self, storage: JobsStorage) -> List[JobRecord]:
        jobs = [
            # no name:
            self._create_pending_job(owner="user1", job_name=None),
            self._create_running_job(owner="user2", job_name=None),
            self._create_succeeded_job(owner="user3", job_name=None),
            self._create_failed_job(owner="user3", job_name=None),
            self._create_cancelled_job(owner="user3", job_name=None),
            # user1, jobname1:
            self._create_succeeded_job(owner="user1", job_name="jobname1"),
            self._create_failed_job(owner="user1", job_name="jobname1"),
            self._create_cancelled_job(owner="user1", job_name="jobname1"),
            self._create_pending_job(owner="user1", job_name="jobname1"),
            # user1, jobname2:
            self._create_succeeded_job(owner="user1", job_name="jobname2"),
            self._create_failed_job(owner="user1", job_name="jobname2"),
            self._create_cancelled_job(owner="user1", job_name="jobname2"),
            self._create_running_job(owner="user1", job_name="jobname2"),
            # user2, jobname2:
            self._create_succeeded_job(owner="user2", job_name="jobname2"),
            self._create_failed_job(owner="user2", job_name="jobname2"),
            self._create_cancelled_job(owner="user2", job_name="jobname2"),
            self._create_pending_job(owner="user2", job_name="jobname2"),
            # user2, jobname3:
            self._create_succeeded_job(owner="user2", job_name="jobname3"),
            self._create_failed_job(owner="user2", job_name="jobname3"),
            self._create_cancelled_job(owner="user2", job_name="jobname3"),
            self._create_running_job(owner="user2", job_name="jobname3"),
            # user3, jobname3:
            self._create_succeeded_job(owner="user3", job_name="jobname3"),
            self._create_failed_job(owner="user3", job_name="jobname3"),
            self._create_cancelled_job(owner="user3", job_name="jobname3"),
            self._create_pending_job(owner="user3", job_name="jobname3"),
        ]
        for job in jobs:
            async with storage.try_create_job(job):
                pass
        return jobs

    @pytest.mark.asyncio
    async def test_get_all_filter_by_single_owner(self, storage: JobsStorage) -> None:
        jobs = await self.prepare_filtering_test(storage)

        owners = {"user1"}
        job_filter = JobFilter(owners=owners)
        job_ids = [job.id for job in await storage.get_all_jobs(job_filter)]
        expected = [job.id for job in jobs if job.owner in owners]
        assert expected
        assert job_ids == expected

    @pytest.mark.asyncio
    async def test_get_all_filter_by_multiple_owners(
        self, storage: JobsStorage
    ) -> None:
        jobs = await self.prepare_filtering_test(storage)

        owners = {"user1", "user3"}
        job_filter = JobFilter(owners=owners)
        job_ids = [job.id for job in await storage.get_all_jobs(job_filter)]
        expected = [job.id for job in jobs if job.owner in owners]
        assert expected
        assert job_ids == expected

    pend = JobStatus.PENDING
    runn = JobStatus.RUNNING
    succ = JobStatus.SUCCEEDED
    canc = JobStatus.CANCELLED
    fail = JobStatus.FAILED

    @pytest.mark.asyncio
    @pytest.mark.parametrize(
        "name,owners,statuses",
        [
            (None, (), ()),
            (None, (), (pend,)),
            (None, (), (pend, runn)),
            (None, (), (succ, fail)),
            (None, (), (succ, fail, runn)),
            (None, (), (succ, fail, runn, pend)),
            (None, (), (succ, canc, fail, runn, pend)),
            (None, ("user1",), ()),
            (None, ("user1",), (pend,)),
            (None, ("user1",), (pend, runn)),
            (None, ("user1",), (succ, fail)),
            (None, ("user1",), (succ, fail, runn, pend)),
            (None, ("user1",), (succ, canc, fail, runn, pend)),
            (None, ("user1", "user2"), ()),
            (None, ("user1", "user2"), (pend,)),
            (None, ("user1", "user2"), (pend, runn)),
            (None, ("user1", "user2"), (succ, fail)),
            (None, ("user1", "user2"), (succ, fail, runn, pend)),
            (None, ("user1", "user2"), (succ, canc, fail, runn, pend)),
            ("jobname1", ("user1",), ()),
            ("jobname1", ("user1",), (pend,)),
            ("jobname1", ("user1",), (pend, runn)),
            ("jobname1", ("user1",), (succ, fail)),
            ("jobname1", ("user1",), (succ, fail, runn, pend)),
            ("jobname1", ("user1",), (succ, canc, fail, runn, pend)),
            ("jobname1", ("user1", "user2"), ()),
            ("jobname1", ("user1", "user2"), (pend,)),
            ("jobname1", ("user1", "user2"), (pend, runn)),
            ("jobname1", ("user1", "user2"), (succ, fail)),
            ("jobname1", ("user1", "user2"), (succ, fail, runn, pend)),
            ("jobname1", ("user1", "user2"), (succ, canc, fail, runn, pend)),
        ],
    )
    async def test_get_all_with_filters(
        self,
        owners: Tuple[str, ...],
        name: Optional[str],
        statuses: Tuple[JobStatus, ...],
        storage: JobsStorage,
    ) -> None:
        def sort_jobs_as_primitives(array: List[JobRecord]) -> List[Dict[str, Any]]:
            return sorted(
                [job.to_primitive() for job in array], key=lambda job: job["id"]
            )

        jobs = await self.prepare_filtering_test(storage)
        job_filter = JobFilter(name=name, owners=set(owners), statuses=set(statuses))
        actual = sort_jobs_as_primitives(await storage.get_all_jobs(job_filter))
        expected = sort_jobs_as_primitives(
            [
                job
                for job in jobs
                if (not name or job.name == name)
                and (not owners or job.owner in owners)
                and (not statuses or job.status in statuses)
            ]
        )
        assert actual == expected

    @pytest.mark.asyncio
    @pytest.mark.parametrize(
        "name,statuses",
        [
            ("jobname1", ()),
            ("jobname1", (pend,)),
            ("jobname1", (pend, runn)),
            ("jobname1", (succ, fail)),
            ("jobname1", (succ, fail, runn)),
            ("jobname1", (succ, fail, runn, pend)),
        ],
    )
    async def test_get_all_filter_by_name_with_no_owner(
        self,
        name: Optional[str],
        statuses: Tuple[JobStatus, ...],
        storage: JobsStorage,
    ) -> None:
        jobs = await self.prepare_filtering_test(storage)
        job_filter = JobFilter(name=name, owners=set(), statuses=set(statuses))
        job_ids = [job.id for job in await storage.get_all_jobs(job_filter)]
        expected = [
            job.id
            for job in jobs
            if job.name == name and (not statuses or job.status in statuses)
        ]
        assert job_ids == expected

    @pytest.mark.asyncio
    async def test_get_all_filter_by_owner_and_name(
        self,
        storage: JobsStorage,
    ) -> None:
        jobs = await self.prepare_filtering_test(storage)

        name = "jobname2"
        owner = "user1"

        job_filter = JobFilter(name=name, owners={owner})
        job_ids = [job.id for job in await storage.get_all_jobs(job_filter)]
        expected = [job.id for job in jobs if job.name == name and job.owner == owner]
        assert job_ids == expected

    @pytest.mark.asyncio
    async def test_get_all_filter_by_owner_name_and_status(
        self,
        storage: JobsStorage,
    ) -> None:
        jobs = await self.prepare_filtering_test(storage)

        name = "jobname2"
        owner = "user1"
        statuses = {JobStatus.RUNNING}
        job_filter = JobFilter(name=name, owners={owner}, statuses=statuses)
        job_ids = [job.id for job in await storage.get_all_jobs(job_filter)]
        expected = [
            job.id
            for job in jobs
            if job.name == name and job.owner == owner and job.status in statuses
        ]
        assert job_ids == expected

        name = "jobname2"
        owner = "user1"
        statuses = {JobStatus.SUCCEEDED}
        job_filter = JobFilter(name=name, owners={owner}, statuses=statuses)
        job_ids = [job.id for job in await storage.get_all_jobs(job_filter)]
        expected = [
            job.id
            for job in jobs
            if job.name == name and job.owner == owner and job.status in statuses
        ]
        assert job_ids == expected

        name = "jobname2"
        owner = "user1"
        statuses = {JobStatus.SUCCEEDED, JobStatus.RUNNING}
        job_filter = JobFilter(name=name, owners={owner}, statuses=statuses)
        job_ids = [job.id for job in await storage.get_all_jobs(job_filter)]
        expected = [
            job.id
            for job in jobs
            if job.name == name and job.owner == owner and job.status in statuses
        ]
        assert job_ids == expected

        name = "jobname3"
        owner = "user2"
        statuses = {JobStatus.FAILED}
        job_filter = JobFilter(name=name, owners={owner}, statuses=statuses)
        job_ids = [job.id for job in await storage.get_all_jobs(job_filter)]
        expected = [
            job.id
            for job in jobs
            if job.name == name and job.owner == owner and job.status in statuses
        ]
        assert job_ids == expected

    @pytest.mark.asyncio
    async def test_get_all_shared_by_name(self, storage: JobsStorage) -> None:
        jobs = await self.prepare_filtering_test(storage)

        job_filter = JobFilter(
            clusters={"test-cluster": {"user1": {"jobname2"}, "user2": {"jobname3"}}},
            owners={"user1", "user2"},
            statuses={JobStatus.FAILED},
        )
        job_ids = [job.id for job in await storage.get_all_jobs(job_filter)]
        expected = [
            job.id
            for job in jobs
            if job.status == JobStatus.FAILED
            and (
                (job.owner == "user1" and job.name == "jobname2")
                or (job.owner == "user2" and job.name == "jobname3")
            )
        ]
        assert job_ids == expected

    @pytest.mark.asyncio
    async def test_get_all_filter_by_hostname(
        self,
        storage: JobsStorage,
    ) -> None:
        jobs = await self.prepare_filtering_test(storage)

        job1, job2 = islice(jobs, 2)

        job_filter = JobFilter(ids={job1.id})
        job_ids = {job.id for job in await storage.get_all_jobs(job_filter)}
        assert job_ids == {job1.id}

        job_filter = JobFilter(ids={job1.id, job2.id})
        job_ids = {job.id for job in await storage.get_all_jobs(job_filter)}
        assert job_ids == {job1.id, job2.id}

    @pytest.mark.asyncio
    async def test_get_all_filter_by_hostname_and_status(
        self,
        storage: JobsStorage,
    ) -> None:
        jobs = await self.prepare_filtering_test(storage)

        running_job_ids = {
            job.id
            for job in islice(
                (job for job in jobs if job.status == JobStatus.RUNNING), 2
            )
        }
        succeeded_job_ids = {
            job.id
            for job in islice(
                (job for job in jobs if job.status == JobStatus.SUCCEEDED), 2
            )
        }

        statuses = {JobStatus.RUNNING}
        job_filter = JobFilter(ids=running_job_ids, statuses=statuses)
        job_ids = {job.id for job in await storage.get_all_jobs(job_filter)}
        assert job_ids == running_job_ids

        statuses = {JobStatus.SUCCEEDED, JobStatus.RUNNING}
        job_filter = JobFilter(ids=running_job_ids, statuses=statuses)
        job_ids = {job.id for job in await storage.get_all_jobs(job_filter)}
        assert job_ids == running_job_ids

        statuses = {JobStatus.SUCCEEDED}
        job_filter = JobFilter(ids=running_job_ids, statuses=statuses)
        job_ids = {job.id for job in await storage.get_all_jobs(job_filter)}
        assert job_ids == set()

        statuses = {JobStatus.FAILED, JobStatus.RUNNING}
        job_filter = JobFilter(
            ids=running_job_ids | succeeded_job_ids, statuses=statuses
        )
        job_ids = {job.id for job in await storage.get_all_jobs(job_filter)}
        assert job_ids == running_job_ids

        statuses = {JobStatus.SUCCEEDED, JobStatus.FAILED, JobStatus.RUNNING}
        job_filter = JobFilter(
            ids=running_job_ids | succeeded_job_ids, statuses=statuses
        )
        job_ids = {job.id for job in await storage.get_all_jobs(job_filter)}
        assert set(job_ids) == running_job_ids | succeeded_job_ids

    async def prepare_filtering_test_different_clusters(
        self, storage: JobsStorage
    ) -> List[JobRecord]:
        jobs = [
            self._create_running_job(owner="user1", cluster_name="test-cluster"),
            self._create_succeeded_job(
                owner="user1", cluster_name="test-cluster", job_name="jobname1"
            ),
            self._create_failed_job(
                owner="user2", cluster_name="test-cluster", job_name="jobname1"
            ),
            self._create_succeeded_job(
                owner="user3", cluster_name="test-cluster", job_name="jobname1"
            ),
            self._create_succeeded_job(owner="user1", cluster_name="my-cluster"),
            self._create_failed_job(owner="user3", cluster_name="my-cluster"),
            self._create_failed_job(owner="user1", cluster_name="other-cluster"),
            self._create_succeeded_job(owner="user2", cluster_name="other-cluster"),
            self._create_running_job(owner="user3", cluster_name="other-cluster"),
        ]
        for job in jobs:
            async with storage.try_create_job(job):
                pass
        return jobs

    @pytest.mark.asyncio
    async def test_get_all_filter_by_cluster(self, storage: JobsStorage) -> None:
        jobs = await self.prepare_filtering_test_different_clusters(storage)

        job_filter = JobFilter(clusters={"test-cluster": {}})
        job_ids = [job.id for job in await storage.get_all_jobs(job_filter)]
        assert job_ids == [job.id for job in jobs[:4]]

        job_filter = JobFilter(clusters={"test-cluster": {}, "my-cluster": {}})
        job_ids = [job.id for job in await storage.get_all_jobs(job_filter)]
        assert job_ids == [job.id for job in jobs[:6]]

        job_filter = JobFilter(clusters={"nonexisting-cluster": {}})
        job_ids = [job.id for job in await storage.get_all_jobs(job_filter)]
        assert job_ids == []

    @pytest.mark.asyncio
    async def test_get_all_filter_by_cluster_and_owner(
        self, storage: JobsStorage
    ) -> None:
        jobs = await self.prepare_filtering_test_different_clusters(storage)

        job_filter = JobFilter(clusters={"test-cluster": {}}, owners={"user1"})
        job_ids = [job.id for job in await storage.get_all_jobs(job_filter)]
        assert job_ids == [jobs[0].id, jobs[1].id]

        job_filter = JobFilter(
            clusters={"test-cluster": {}, "my-cluster": {}}, owners={"user1"}
        )
        job_ids = [job.id for job in await storage.get_all_jobs(job_filter)]
        assert job_ids == [jobs[0].id, jobs[1].id, jobs[4].id]

        job_filter = JobFilter(clusters={"test-cluster": {}}, owners={"user1", "user2"})
        job_ids = [job.id for job in await storage.get_all_jobs(job_filter)]
        assert job_ids == [jobs[0].id, jobs[1].id, jobs[2].id]

        job_filter = JobFilter(clusters={"my-cluster": {}}, owners={"user2"})
        job_ids = [job.id for job in await storage.get_all_jobs(job_filter)]
        assert job_ids == []

        job_filter = JobFilter(
            clusters={
                "test-cluster": {"user1": set()},
                "other-cluster": {"user2": set()},
            },
            owners={"user1", "user2"},
        )
        job_ids = [job.id for job in await storage.get_all_jobs(job_filter)]
        assert job_ids == [jobs[0].id, jobs[1].id, jobs[7].id]

        job_filter = JobFilter(
            clusters={"test-cluster": {"user1": set()}, "other-cluster": {}},
            owners={"user1", "user2"},
        )
        job_ids = [job.id for job in await storage.get_all_jobs(job_filter)]
        assert job_ids == [jobs[0].id, jobs[1].id, jobs[6].id, jobs[7].id]

        job_filter = JobFilter(
            clusters={
                "test-cluster": {"user1": set(), "user2": set()},
                "my-cluster": {"user2": set(), "user3": set()},
                "other-cluster": {"user2": set()},
            },
            owners={"user1", "user2", "user3"},
        )
        job_ids = [job.id for job in await storage.get_all_jobs(job_filter)]
        assert job_ids == [jobs[0].id, jobs[1].id, jobs[2].id, jobs[5].id, jobs[7].id]

        job_filter = JobFilter(
            clusters={"test-cluster": {"user1": set()}, "other-cluster": {}},
        )
        job_ids = [job.id for job in await storage.get_all_jobs(job_filter)]
        assert job_ids == [jobs[0].id, jobs[1].id, jobs[6].id, jobs[7].id, jobs[8].id]

    @pytest.mark.asyncio
    async def test_get_all_filter_by_cluster_and_name(
        self, storage: JobsStorage
    ) -> None:
        jobs = await self.prepare_filtering_test_different_clusters(storage)

        job_filter = JobFilter(
            clusters={"test-cluster": {}}, owners={"user1", "user2"}, name="jobname1"
        )
        job_ids = [job.id for job in await storage.get_all_jobs(job_filter)]
        assert job_ids == [jobs[1].id, jobs[2].id]

        job_filter = JobFilter(clusters={"test-cluster": {}}, name="jobname1")
        job_ids = [job.id for job in await storage.get_all_jobs(job_filter)]
        assert job_ids == [jobs[1].id, jobs[2].id, jobs[3].id]

        job_filter = JobFilter(
            clusters={"test-cluster": {}}, owners={"user1", "user2"}, name="jobname2"
        )
        job_ids = [job.id for job in await storage.get_all_jobs(job_filter)]
        assert job_ids == []

        job_filter = JobFilter(clusters={"test-cluster": {}}, name="jobname2")
        job_ids = [job.id for job in await storage.get_all_jobs(job_filter)]
        assert job_ids == []

    @pytest.mark.asyncio
    async def test_get_all_filter_by_cluster_and_status(
        self, storage: JobsStorage
    ) -> None:
        jobs = await self.prepare_filtering_test_different_clusters(storage)

        job_filter = JobFilter(
            clusters={"test-cluster": {}}, statuses={JobStatus.SUCCEEDED}
        )
        job_ids = [job.id for job in await storage.get_all_jobs(job_filter)]
        assert job_ids == [jobs[1].id, jobs[3].id]

        job_filter = JobFilter(
            clusters={"test-cluster": {}, "my-cluster": {}},
            statuses={JobStatus.SUCCEEDED},
        )
        job_ids = [job.id for job in await storage.get_all_jobs(job_filter)]
        assert job_ids == [jobs[1].id, jobs[3].id, jobs[4].id]

        job_filter = JobFilter(
            clusters={"test-cluster": {}},
            statuses={JobStatus.RUNNING, JobStatus.SUCCEEDED},
        )
        job_ids = [job.id for job in await storage.get_all_jobs(job_filter)]
        assert job_ids == [jobs[0].id, jobs[1].id, jobs[3].id]

        job_filter = JobFilter(
            clusters={"my-cluster": {}}, statuses={JobStatus.RUNNING}
        )
        job_ids = [job.id for job in await storage.get_all_jobs(job_filter)]
        assert job_ids == []

    @pytest.mark.asyncio
    async def test_get_running_empty(self, storage: JobsStorage) -> None:

        jobs = await storage.get_running_jobs()
        assert not jobs

    @pytest.mark.asyncio
    async def test_get_running(self, storage: JobsStorage) -> None:
        pending_job = self._create_pending_job()
        running_job = self._create_running_job()
        succeeded_job = self._create_succeeded_job()
        await storage.set_job(pending_job)
        await storage.set_job(running_job)
        await storage.set_job(succeeded_job)

        jobs = await storage.get_running_jobs()
        assert len(jobs) == 1
        job = jobs[0]
        assert job.id == running_job.id
        assert job.status == JobStatus.RUNNING

    @pytest.mark.asyncio
    async def test_get_unfinished_empty(self, storage: JobsStorage) -> None:
        jobs = await storage.get_unfinished_jobs()
        assert not jobs

    @pytest.mark.asyncio
    async def test_get_unfinished(self, storage: JobsStorage) -> None:
        pending_job = self._create_pending_job()
        running_job = self._create_running_job()
        succeeded_job = self._create_succeeded_job()
        await storage.set_job(pending_job)
        await storage.set_job(running_job)
        await storage.set_job(succeeded_job)

        jobs = await storage.get_unfinished_jobs()
        assert len(jobs) == 2
        assert [job.id for job in jobs] == [pending_job.id, running_job.id]
        assert all([not job.is_finished for job in jobs])

    @pytest.mark.asyncio
    async def test_get_for_deletion_empty(self, storage: JobsStorage) -> None:
        jobs = await storage.get_jobs_for_deletion()
        assert not jobs

    @pytest.mark.asyncio
    async def test_get_for_deletion(self, storage: JobsStorage) -> None:
        pending_job = self._create_pending_job()
        running_job = self._create_running_job()
        succeeded_job = self._create_succeeded_job()
        deleted_job = self._create_succeeded_job(materialized=False)
        await storage.set_job(pending_job)
        await storage.set_job(running_job)
        await storage.set_job(succeeded_job)
        await storage.set_job(deleted_job)

        jobs = await storage.get_jobs_for_deletion()
        assert len(jobs) == 1
        job = jobs[0]
        assert job.id == succeeded_job.id
        assert job.status == JobStatus.SUCCEEDED
        assert job.materialized

    @pytest.mark.asyncio
    async def test_get_tags_empty(self, storage: JobsStorage) -> None:
        for job in [
            self._create_job(owner="u", tags=["b"]),
            self._create_job(owner="u", tags=["a"]),
            self._create_job(owner="u", tags=()),
        ]:
            async with storage.try_create_job(job):
                pass

        tags_u1 = await storage.get_tags("another")
        assert tags_u1 == []

    @pytest.mark.asyncio
    async def test_get_tags_single(self, storage: JobsStorage) -> None:
        f1 = lambda: datetime(year=2020, month=1, day=1, second=1)  # noqa
        f2 = lambda: datetime(year=2020, month=1, day=1, second=2)  # noqa
        f3 = lambda: datetime(year=2020, month=1, day=1, second=3)  # noqa

        for job in [
            self._create_job(owner="u", current_datetime_factory=f1, tags=["b"]),
            self._create_job(owner="u", current_datetime_factory=f2, tags=["a"]),
            self._create_job(owner="u", current_datetime_factory=f3, tags=["c"]),
            self._create_job(owner="u", tags=()),
        ]:
            async with storage.try_create_job(job):
                pass

        tags_u1 = await storage.get_tags("u")
        assert tags_u1 == ["c", "a", "b"]

    @pytest.mark.asyncio
    async def test_get_tags_multiple(self, storage: JobsStorage) -> None:
        f1 = lambda: datetime(year=2020, month=1, day=1, second=1)  # noqa
        f2 = lambda: datetime(year=2020, month=1, day=1, second=2)  # noqa

        for job in [
            self._create_job(
                owner="u", current_datetime_factory=f1, tags=["b", "a", "c"]
            ),
            self._create_job(owner="u", current_datetime_factory=f2, tags=["d"]),
            self._create_job(owner="u", tags=()),
        ]:
            async with storage.try_create_job(job):
                pass

        tags_u1 = await storage.get_tags("u")
        assert tags_u1 == ["d", "a", "b", "c"]

    @pytest.mark.asyncio
    async def test_get_tags_overwrite_single(self, storage: JobsStorage) -> None:
        f1 = lambda: datetime(year=2020, month=1, day=1, second=1)  # noqa
        f2 = lambda: datetime(year=2020, month=1, day=1, second=2)  # noqa
        f3 = lambda: datetime(year=2020, month=1, day=1, second=3)  # noqa
        f4 = lambda: datetime(year=2020, month=1, day=1, second=4)  # noqa

        for job in [
            self._create_job(owner="u", current_datetime_factory=f1, tags=["a"]),
            self._create_job(owner="u", current_datetime_factory=f2, tags=["b"]),
            self._create_job(owner="u", current_datetime_factory=f3, tags=["a"]),
            self._create_job(owner="u", current_datetime_factory=f4, tags=["c"]),
            self._create_job(owner="u", tags=()),
        ]:
            async with storage.try_create_job(job):
                pass

        tags_u1 = await storage.get_tags("u")
        assert tags_u1 == ["c", "a", "b"]

    @pytest.mark.asyncio
    async def test_get_tags_overwrite_multiple(
        self,
        storage: JobsStorage,
    ) -> None:
        f1 = lambda: datetime(year=2020, month=1, day=1, second=1)  # noqa
        f2 = lambda: datetime(year=2020, month=1, day=1, second=2)  # noqa
        f3 = lambda: datetime(year=2020, month=1, day=1, second=3)  # noqa

        for job in [
            self._create_job(owner="u", current_datetime_factory=f1, tags=["a"]),
            self._create_job(owner="u", current_datetime_factory=f2, tags=["b"]),
            self._create_job(owner="u", current_datetime_factory=f3, tags=["c", "a"]),
            self._create_job(owner="u", tags=()),
        ]:
            async with storage.try_create_job(job):
                pass

        tags_u1 = await storage.get_tags("u")
        assert tags_u1 == ["a", "c", "b"]

    @pytest.mark.asyncio
    async def test_job_lifecycle(self, storage: JobsStorage) -> None:
        job = self._create_pending_job()
        job_id = job.id
        await storage.set_job(job)

        jobs = await storage.get_all_jobs()
        assert len(jobs) == 1
        job = jobs[0]
        assert job.id == job_id
        assert job.status == JobStatus.PENDING

        jobs = await storage.get_running_jobs()
        assert not jobs

        jobs = await storage.get_jobs_for_deletion()
        assert not jobs

        job.materialized = True
        job.status = JobStatus.RUNNING
        await storage.set_job(job)

        jobs = await storage.get_all_jobs()
        assert len(jobs) == 1

        jobs = await storage.get_running_jobs()
        assert len(jobs) == 1
        job = jobs[0]
        assert job.id == job_id
        assert job.status == JobStatus.RUNNING

        jobs = await storage.get_jobs_for_deletion()
        assert not jobs

        job.status = JobStatus.FAILED
        await storage.set_job(job)

        jobs = await storage.get_all_jobs()
        assert len(jobs) == 1

        jobs = await storage.get_running_jobs()
        assert not jobs

        jobs = await storage.get_jobs_for_deletion()
        assert len(jobs) == 1
        job = jobs[0]
        assert job.id == job_id
        assert job.status == JobStatus.FAILED
        assert job.materialized

        job.materialized = False
        await storage.set_job(job)

        jobs = await storage.get_all_jobs()
        assert len(jobs) == 1

        jobs = await storage.get_running_jobs()
        assert not jobs

        jobs = await storage.get_jobs_for_deletion()
        assert not jobs

    @pytest.mark.asyncio
    async def test_try_update_job__no_name__ok(self, storage: JobsStorage) -> None:
        pending_job = self._create_pending_job()
        await storage.set_job(pending_job)

        async with storage.try_update_job(pending_job.id) as job:
            assert pending_job.status == JobStatus.PENDING
            job.status = JobStatus.RUNNING

        running_job = await storage.get_job(pending_job.id)
        assert running_job.status == JobStatus.RUNNING

    @pytest.mark.asyncio
    async def test_try_update_job__not_found(self, storage: JobsStorage) -> None:
        pending_job = self._create_pending_job()

        with pytest.raises(JobError, match=f"no such job {pending_job.id}"):
            async with storage.try_update_job(pending_job.id):
                pass

    @pytest.mark.asyncio
    async def test_try_update_job__no_name__job_changed_while_creation(
        self, storage: JobsStorage
    ) -> None:
        job = self._create_pending_job()
        await storage.set_job(job)

        # process-1
        with pytest.raises(
            JobStorageTransactionError, match=f"Job {{id={job.id}}} has changed"
        ):
            async with storage.try_update_job(job.id) as first_job:
                # value in orchestrator: PENDING, value in redis: None
                assert first_job.status == JobStatus.PENDING
                first_job.status = JobStatus.SUCCEEDED
                # value in orchestrator: SUCCEEDED, value in redis: None

                # process-2
                with not_raises(JobStorageTransactionError):
                    async with storage.try_update_job(job.id) as second_job:
                        # value in orchestrator: succeeded, value in redis: None
                        assert second_job.status == JobStatus.PENDING
                        second_job.status = JobStatus.RUNNING
                        # value in orchestrator: FAILED, value in redis: None
                        # now status FAILED is written into the redis by process-2

                # now status SUCCEEDED fails to be written into the redis by process-1

                # now jobs-service catches transaction exception thrown
                # by process-1 and deletes the newly created by process-1 job.

        result_job = await storage.get_job(job.id)
        assert result_job.status == JobStatus.RUNNING

    @pytest.mark.asyncio
    async def test_try_update_job__different_name_same_owner__ok(
        self, storage: JobsStorage
    ) -> None:

        owner = "test-user-1"
        job_name_1 = "some-test-job-name-1"
        job_name_2 = "some-test-job-name-2"

        job_1 = self._create_pending_job(job_name=job_name_1, owner=owner)
        job_2 = self._create_running_job(job_name=job_name_2, owner=owner)

        await storage.set_job(job_1)
        await storage.set_job(job_2)

        async with storage.try_update_job(job_1.id) as job_1_current:
            async with storage.try_update_job(job_2.id) as job_2_current:
                job_2_current.status = JobStatus.FAILED
            job_1_current.status = JobStatus.SUCCEEDED

        job = await storage.get_job(job_1.id)
        assert job.id == job_1.id
        assert job.status == JobStatus.SUCCEEDED

        job = await storage.get_job(job_2.id)
        assert job.id == job_2.id
        assert job.status == JobStatus.FAILED

    @pytest.mark.asyncio
    async def test_try_update_job__same_name_different_owner__ok(
        self, storage: JobsStorage
    ) -> None:

        owner_1 = "test-user-1"
        owner_2 = "test-user-2"
        job_name = "some-test-job-name"

        job_1 = self._create_pending_job(job_name=job_name, owner=owner_1)
        job_2 = self._create_running_job(job_name=job_name, owner=owner_2)

        await storage.set_job(job_1)
        await storage.set_job(job_2)

        async with storage.try_update_job(job_1.id) as job_1_current:
            async with storage.try_update_job(job_2.id) as job_2_current:
                job_2_current.status = JobStatus.FAILED
            job_1_current.status = JobStatus.SUCCEEDED

        job = await storage.get_job(job_1.id)
        assert job.id == job_1.id
        assert job.status == JobStatus.SUCCEEDED

        job = await storage.get_job(job_2.id)
        assert job.id == job_2.id
        assert job.status == JobStatus.FAILED

    @pytest.mark.asyncio
    @pytest.mark.parametrize(
        "first_job_status", [JobStatus.SUCCEEDED, JobStatus.FAILED]
    )
    async def test_try_update_job__same_name_with_a_terminated_job__ok(
        self, storage: JobsStorage, first_job_status: JobStatus
    ) -> None:
        owner = "test-user"
        job_name = "some-test-job-name"

        first_job = self._create_job(
            name=job_name, status=first_job_status, owner=owner
        )
        second_job = self._create_pending_job(owner=owner, job_name=job_name)

        # create first job:
        async with storage.try_create_job(first_job):
            pass
        job = await storage.get_job(first_job.id)
        assert job.id == first_job.id
        assert job.status == first_job_status

        # create second job:
        async with storage.try_create_job(second_job):
            pass
        job = await storage.get_job(second_job.id)
        assert job.id == second_job.id
        assert job.status == JobStatus.PENDING

        # update second job:
        async with storage.try_update_job(second_job.id) as job:
            job.status = JobStatus.RUNNING
        job = await storage.get_job(second_job.id)
        assert job.id == second_job.id
        assert job.status == JobStatus.RUNNING

    @pytest.mark.asyncio
    async def test_migrate_no_jobs(self, redis_client: aioredis.Redis) -> None:
        storage = RedisJobsStorage(client=redis_client)

        jobs = await storage.get_all_jobs()
        assert not jobs

        assert await storage.migrate()
        assert not await storage.migrate()

    @pytest.mark.asyncio
    async def test_migrate_version_4(self, redis_client: aioredis.Redis) -> None:
        job = self._create_succeeded_job()

        storage = RedisJobsStorage(client=redis_client)
        async with storage.try_create_job(job, skip_index=True):
            pass

        jobs_for_deletion = await storage.get_jobs_for_deletion()
        assert not jobs_for_deletion

        await storage.migrate()

        jobs_for_deletion = await storage.get_jobs_for_deletion()
        assert jobs_for_deletion

        assert not await storage.migrate()

    @pytest.mark.asyncio
    async def test_migrate(self, redis_client: aioredis.Redis) -> None:
        first_job = self._create_pending_job(owner="testuser")
        second_job = self._create_running_job(owner="testuser")

        first_job.allow_empty_cluster_name = True
        first_job.cluster_name = ""
        second_job.allow_empty_cluster_name = True
        second_job.cluster_name = ""

        storage = RedisJobsStorage(client=redis_client)
        async with storage.try_create_job(first_job, skip_index=True):
            pass
        async with storage.try_create_job(second_job, skip_index=True):
            pass

        jobs = await storage.get_all_jobs()
        job_ids = [job.id for job in jobs]
        assert not job_ids

        filters = JobFilter(owners={"testuser"})

        jobs = await storage.get_all_jobs(filters)
        assert not jobs

        filters2 = JobFilter(clusters={"default": {}})

        jobs = await storage.get_all_jobs(filters2)
        assert not jobs

        assert await storage.migrate()

        jobs = await storage.get_all_jobs(filters)
        job_ids = [job.id for job in jobs]
        assert job_ids == [first_job.id, second_job.id]
        for job in jobs:
            assert job.cluster_name == "default"

        jobs = await storage.get_all_jobs(filters2)
        job_ids = [job.id for job in jobs]
        assert job_ids == [first_job.id, second_job.id]

        assert not await storage.migrate()

    @pytest.mark.asyncio
    async def test_migrate_version_6(self, redis_client: aioredis.Redis) -> None:
        job = self._create_running_job(owner="testuser")
        volume1 = ContainerVolume(
            uri=URL("storage://path/to/dir"),
            dst_path=PurePath("/container/path"),
            read_only=True,
        )
        volume2 = ContainerVolume(
            uri=URL("storage://testuser/path/to/dir2"),
            dst_path=PurePath("/container/path2"),
            read_only=True,
        )
        volume3 = ContainerVolume(
            uri=URL("storage://test-cluster/testuser/path/to/dir3"),
            dst_path=PurePath("/container/path3"),
            read_only=True,
        )
        volume4 = ContainerVolume(
            uri=URL(""),
            dst_path=PurePath("/container/path4"),
            read_only=True,
        )
        job.request.container.volumes.extend([volume1, volume2, volume3, volume4])

        storage = RedisJobsStorage(client=redis_client)
        async with storage.try_create_job(job):
            pass

        original_job = await storage.get_job(job.id)
        assert original_job.request.container.volumes == [
            volume1,
            volume2,
            volume3,
            volume4,
        ]

        assert await storage.migrate()

        migrated_job = await storage.get_job(job.id)
        migrated_volume1 = ContainerVolume(
            uri=URL("storage://test-cluster/path/to/dir"),
            dst_path=PurePath("/container/path"),
            read_only=True,
        )
        migrated_volume2 = ContainerVolume(
            uri=URL("storage://test-cluster/testuser/path/to/dir2"),
            dst_path=PurePath("/container/path2"),
            read_only=True,
        )
        assert migrated_job.request.container.volumes == [
            migrated_volume1,
            migrated_volume2,
            volume3,
            volume4,
        ]

        assert not await storage.migrate()

    def test_migrate_storage_uri(self) -> None:
        convert = RedisJobsStorage._migrate_storage_uri
        assert convert(URL("storage://alice"), "test-cluster") == URL(
            "storage://test-cluster/alice"
        )
        assert convert(URL("storage://bob/data"), "test-cluster") == URL(
            "storage://test-cluster/bob/data"
        )
        assert convert(URL("storage://"), "test-cluster") == URL(
            "storage://test-cluster/"
        )
        assert convert(URL("storage:"), "test-cluster") == URL(
            "storage://test-cluster/"
        )
        with pytest.raises(AssertionError):
            convert(URL("storage://test-cluster/alice"), "test-cluster")
        with pytest.raises(AssertionError):
            convert(URL("image://alice"), "test-cluster")

    @pytest.mark.asyncio
    async def test_get_aggregated_run_time_for_user(self, storage: JobsStorage) -> None:
        test_started_at = current_datetime_factory()
        owner = f"test-user-{random_str()}"

        job_started_delay = timedelta(hours=1)
        time_pending_delta = timedelta(minutes=3)
        time_running_delta = timedelta(minutes=20)

        job_started_at = current_datetime_factory() - job_started_delay
        job_running_at = job_started_at + time_pending_delta
        job_finished_at = job_running_at + time_running_delta

        expected_alive_job_runtime = job_started_delay - time_pending_delta
        expected_finished_job_runtime = time_running_delta

        def create_job(cluster_name: str, with_gpu: bool, finished: bool) -> JobRecord:
            status_history = [
                JobStatusItem.create(JobStatus.PENDING, transition_time=job_started_at),
                JobStatusItem.create(JobStatus.RUNNING, transition_time=job_running_at),
            ]
            if finished:
                status_history.append(
                    JobStatusItem.create(
                        JobStatus.SUCCEEDED, transition_time=job_finished_at
                    )
                )
            return JobRecord.create(
                owner=owner,
                request=self._create_job_request(with_gpu),
                cluster_name=cluster_name,
                status_history=JobStatusHistory(status_history),
            )

        jobs_with_gpu = [
            create_job(cluster_name="test-cluster1", with_gpu=True, finished=False),
            create_job(cluster_name="test-cluster1", with_gpu=True, finished=False),
            create_job(cluster_name="test-cluster1", with_gpu=True, finished=True),
            create_job(cluster_name="test-cluster1", with_gpu=True, finished=True),
            create_job(cluster_name="test-cluster2", with_gpu=True, finished=False),
            create_job(cluster_name="test-cluster2", with_gpu=True, finished=False),
            create_job(cluster_name="test-cluster2", with_gpu=True, finished=True),
            create_job(cluster_name="test-cluster2", with_gpu=True, finished=True),
        ]
        jobs_no_gpu = [
            create_job(cluster_name="test-cluster1", with_gpu=False, finished=False),
            create_job(cluster_name="test-cluster1", with_gpu=False, finished=False),
            create_job(cluster_name="test-cluster1", with_gpu=False, finished=True),
            create_job(cluster_name="test-cluster1", with_gpu=False, finished=True),
            create_job(cluster_name="test-cluster2", with_gpu=False, finished=False),
            create_job(cluster_name="test-cluster2", with_gpu=False, finished=False),
            create_job(cluster_name="test-cluster2", with_gpu=False, finished=True),
            create_job(cluster_name="test-cluster2", with_gpu=False, finished=True),
        ]
        for job in jobs_with_gpu + jobs_no_gpu:
            async with storage.try_create_job(job):
                pass

        actual_run_time = await storage.get_aggregated_run_time(owner)
        actual_run_times = await storage.get_aggregated_run_time_by_clusters(owner)

        test_elapsed = current_datetime_factory() - test_started_at

        # 4x terminated GPU jobs, 4x GPU alive jobs
        expected = 4 * expected_alive_job_runtime + 4 * expected_finished_job_runtime
        actual_gpu = actual_run_time.total_gpu_run_time_delta
        actual_non_gpu = actual_run_time.total_non_gpu_run_time_delta

        actual_run_time1 = actual_run_times["test-cluster1"]
        expected1 = 2 * expected_alive_job_runtime + 2 * expected_finished_job_runtime
        actual_gpu1 = actual_run_time1.total_gpu_run_time_delta
        actual_non_gpu1 = actual_run_time1.total_non_gpu_run_time_delta

        actual_run_time2 = actual_run_times["test-cluster2"]
        expected2 = 2 * expected_alive_job_runtime + 2 * expected_finished_job_runtime
        actual_gpu2 = actual_run_time2.total_gpu_run_time_delta
        actual_non_gpu2 = actual_run_time2.total_non_gpu_run_time_delta

        # NOTE (ajuszkowski, 4-Apr-2019) Because we don't serialize all fields of `Job`
        # (specifically, `Job.current_datetime_factory`, see issue #560),
        # all deserialized `Job` instances get the default value of
        # `current_datetime_factory`, so we cannot assert exact value
        # of `Job.get_run_time()` in this test
        # 4x running jobs -> 4 * test_elapsed
        assert expected <= actual_gpu <= expected + 4 * test_elapsed
        assert expected <= actual_non_gpu <= expected + 4 * test_elapsed

        # 2x running jobs -> 2 * test_elapsed
        assert expected1 <= actual_gpu1 <= expected1 + 2 * test_elapsed
        assert expected1 <= actual_non_gpu1 <= expected1 + 2 * test_elapsed

        # 2x running jobs -> 2 * test_elapsed
        assert expected2 <= actual_gpu2 <= expected2 + 2 * test_elapsed
        assert expected2 <= actual_non_gpu2 <= expected2 + 2 * test_elapsed

    @pytest.mark.asyncio
    async def test_get_aggregated_run_time_caching(self, storage: JobsStorage) -> None:
        started_at = current_datetime_factory() - timedelta(hours=10)
        owner = f"test-user-{random_str()}"

        async def add_job(
            running_at: datetime,
            with_gpu: bool = False,
            finished_at: Optional[datetime] = None,
            cluster_name: str = "test-cluster",
        ) -> timedelta:
            status_history = [
                JobStatusItem.create(JobStatus.PENDING, transition_time=started_at),
                JobStatusItem.create(JobStatus.RUNNING, transition_time=running_at),
            ]
            if finished_at:
                status_history.append(
                    JobStatusItem.create(
                        JobStatus.SUCCEEDED, transition_time=finished_at
                    )
                )
            job = JobRecord.create(
                owner=owner,
                request=self._create_job_request(with_gpu),
                cluster_name=cluster_name,
                status_history=JobStatusHistory(status_history),
            )
            async with storage.try_create_job(job):
                pass
            return job.get_run_time()

        # No jobs
        result = await storage.get_aggregated_run_time_by_clusters(owner)
        assert result == {}

        # Add completed jobs (gpu and non gpu)
        await add_job(
            started_at, finished_at=started_at + timedelta(hours=1), with_gpu=False
        )
        await add_job(
            started_at, finished_at=started_at + timedelta(hours=2), with_gpu=True
        )

        result = await storage.get_aggregated_run_time_by_clusters(owner)
        assert result["test-cluster"].total_non_gpu_run_time_delta == timedelta(hours=1)
        assert result["test-cluster"].total_gpu_run_time_delta == timedelta(hours=2)

        # Add some more completed jobs
        await add_job(
            started_at + timedelta(hours=2),
            finished_at=started_at + timedelta(hours=4),
            with_gpu=False,
        )
        await add_job(
            started_at + timedelta(hours=2),
            finished_at=started_at + timedelta(hours=3),
            with_gpu=True,
        )

        result = await storage.get_aggregated_run_time_by_clusters(owner)
        assert result["test-cluster"].total_non_gpu_run_time_delta == timedelta(hours=3)
        assert result["test-cluster"].total_gpu_run_time_delta == timedelta(hours=3)

        # Add job that completed seconds ago
        await add_job(
            started_at,
            finished_at=started_at + timedelta(hours=9, seconds=3590),
            with_gpu=False,
        )

        result = await storage.get_aggregated_run_time_by_clusters(owner)
        assert result["test-cluster"].total_non_gpu_run_time_delta == timedelta(
            hours=12, seconds=3590
        )
        assert result["test-cluster"].total_gpu_run_time_delta == timedelta(hours=3)

        # Add running jobs
        await add_job(started_at + timedelta(hours=9), with_gpu=False)
        await add_job(started_at + timedelta(hours=9), with_gpu=True)

        result = await storage.get_aggregated_run_time_by_clusters(owner)
        assert result["test-cluster"].total_non_gpu_run_time_delta >= timedelta(
            hours=13, seconds=3590
        )
        assert result["test-cluster"].total_gpu_run_time_delta >= timedelta(hours=4)

    @pytest.mark.asyncio
    async def test_get_jobs_by_ids_missing_only(self, storage: JobsStorage) -> None:
        jobs = await storage.get_jobs_by_ids({"missing"})
        assert not jobs

    @pytest.mark.asyncio
    async def test_get_jobs_by_ids(self, storage: JobsStorage) -> None:
        first_job = self._create_pending_job(owner="testuser")
        second_job = self._create_running_job(owner="anotheruser")
        third_job = self._create_running_job(owner="testuser")
        for job in (first_job, second_job, third_job):
            async with storage.try_create_job(job):
                pass

        job_filter = JobFilter(statuses={JobStatus.PENDING}, owners={"testuser"})
        jobs = await storage.get_jobs_by_ids(
            {first_job.id, "missing", second_job.id, third_job.id},
            job_filter=job_filter,
        )
        job_ids = [job.id for job in jobs]
        assert job_ids == [first_job.id]

        jobs = await storage.get_jobs_by_ids(
            set(),
            job_filter=job_filter,
        )
        assert jobs == []

    def _fix_utf8(self, value: str) -> str:
        return value.encode("utf-8", "replace").decode("utf-8")

    def check_same_job_sets(
        self, jobs1: List[JobRecord], jobs2: List[JobRecord]
    ) -> None:
        assert len(jobs1) == len(jobs2)
        for job1 in jobs1:
            try:
                job2 = next(job for job in jobs2 if job.id == job1.id)
            except StopIteration:
                raise AssertionError(f"Job with id {job1.id} is missing in job2 set!")
            job1_primitive = job1.to_primitive()
            job2_primitive = job2.to_primitive()
            # If data contains bad utf8 string, it is ok to replace it with ?
            job1_primitive["request"]["container"]["command"] = self._fix_utf8(
                job1_primitive["request"]["container"]["command"]
            )
            job2_primitive["request"]["container"]["command"] = self._fix_utf8(
                job2_primitive["request"]["container"]["command"]
            )
            assert job1_primitive == job2_primitive

    async def prepare_jobs_for_data_migration(
        self, storage: JobsStorage, include_bad_utf8: bool = False
    ) -> List[JobRecord]:
        jobs = [
            self._create_running_job(owner="user1", cluster_name="test-cluster"),
            self._create_succeeded_job(
                owner="user1", cluster_name="test-cluster", job_name="jobname1"
            ),
            self._create_failed_job(
                owner="user2", cluster_name="test-cluster", job_name="jobname1"
            ),
            self._create_succeeded_job(
                owner="user3", cluster_name="test-cluster", job_name="jobname1"
            ),
            self._create_succeeded_job(owner="user1", cluster_name="my-cluster"),
            self._create_failed_job(owner="user3", cluster_name="my-cluster"),
            self._create_failed_job(owner="user1", cluster_name="other-cluster"),
            self._create_succeeded_job(owner="user2", cluster_name="other-cluster"),
            self._create_running_job(owner="user3", cluster_name="other-cluster"),
        ]
        if include_bad_utf8:
            job_bad_command_str = self._create_running_job(
                owner="user-bad-utf-8", cluster_name="test-cluster"
            )
            job_primitive = job_bad_command_str.to_primitive()
            # Some value from wild nature:
            job_primitive["request"]["container"][
                "command"
            ] = "python -c 'print('\"'\"'wandb: Starting wandb agent \ud83d️'\"'\"')'"
            jobs += [JobRecord.from_primitive(job_primitive)]

        for job in jobs:
            async with storage.try_create_job(job):
                pass
        return jobs

    @pytest.mark.asyncio
    async def test_data_migration_from_redis_to_postgres(
        self, redis_config: RedisConfig, redis_client: aioredis.Redis, postgres_dsn: str
    ) -> None:
        # Load data to redis
        redis_storage = RedisJobsStorage(redis_client)
        some_jobs = await self.prepare_jobs_for_data_migration(
            redis_storage, include_bad_utf8=True
        )

        # Setup postgres and run migrations
        postgres_config = PostgresConfig(
            postgres_dsn=postgres_dsn,
            alembic=EnvironConfigFactory().create_alembic(
                postgres_dsn, redis_config.uri
            ),
        )
        migration_runner = MigrationRunner(postgres_config)
        await migration_runner.upgrade()

        try:
            # Check that postgres contains same data
            async with create_postgres_pool(postgres_config) as pool:
                postgres_storage = PostgresJobsStorage(pool)
                jobs_in_postgres = []
                async for job in postgres_storage.iter_all_jobs():
                    jobs_in_postgres.append(job)
                self.check_same_job_sets(some_jobs, jobs_in_postgres)
        finally:
            # Downgrade
            await migration_runner.downgrade()

    @pytest.mark.asyncio
    async def test_data_migration_from_postgres_to_redis(
        self, redis_config: RedisConfig, redis_client: aioredis.Redis, postgres_dsn: str
    ) -> None:

        # Setup postgres and run migrations
        postgres_config = PostgresConfig(
            postgres_dsn=postgres_dsn,
            alembic=EnvironConfigFactory().create_alembic(
                postgres_dsn, redis_config.uri
            ),
        )
        migration_runner = MigrationRunner(postgres_config)
        await migration_runner.upgrade()

        try:
            # Load data to postges
            async with create_postgres_pool(postgres_config) as pool:
                postgres_storage = PostgresJobsStorage(pool)
                some_jobs = await self.prepare_jobs_for_data_migration(postgres_storage)
        finally:
            # Downgrade
            await migration_runner.downgrade()

        # Check that redis contains same data
        redis_storage = RedisJobsStorage(redis_client)
        jobs_in_redis = []
        async for job in redis_storage.iter_all_jobs():
            jobs_in_redis.append(job)
        self.check_same_job_sets(some_jobs, jobs_in_redis)

    @pytest.mark.asyncio
    async def test_data_migration_rename_as_deleted(
        self, redis_config: RedisConfig, postgres_dsn: str
    ) -> None:
        postgres_config = PostgresConfig(
            postgres_dsn=postgres_dsn,
            alembic=EnvironConfigFactory().create_alembic(
                postgres_dsn, redis_config.uri
            ),
        )
        migration_runner = MigrationRunner(postgres_config)
        await migration_runner.upgrade("81675d81a9c7")

        job_not_delete = self._create_failed_job(
            owner="user3", cluster_name="my-cluster", materialized=True
        )
        job_delete = self._create_failed_job(
            owner="user3", cluster_name="my-cluster", materialized=False
        )

        real_to_primitive = JobRecord.to_primitive

        def _to_primitive(self: JobRecord) -> Dict[str, Any]:
            payload = real_to_primitive(self)
            payload["is_deleted"] = not payload["materialized"]
            return payload

        JobRecord.to_primitive = _to_primitive  # type: ignore

        try:
            # Load data to postgres
            async with create_postgres_pool(postgres_config) as pool:
                postgres_storage = PostgresJobsStorage(pool)
                await postgres_storage.set_job(job_delete)
                await postgres_storage.set_job(job_not_delete)
            await migration_runner.upgrade()
            JobRecord.to_primitive = real_to_primitive  # type: ignore
            async with create_postgres_pool(postgres_config) as pool:
                postgres_storage = PostgresJobsStorage(pool)
                assert (await postgres_storage.get_job(job_not_delete.id)).materialized
                assert not (await postgres_storage.get_job(job_delete.id)).materialized
        finally:
            JobRecord.to_primitive = real_to_primitive  # type: ignore
            # Downgrade
            await migration_runner.downgrade()
